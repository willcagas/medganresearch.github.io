<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Accepted paper into ACCV 2024 GAISynMeD Workshop"> 
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Medical Imaging Complexity and its Effects on GAN Performance</title>

  <!-- Global site tag (gtag.js) - Google Analytics
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Medical Imaging Complexity and its Effects on GAN Performance</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://orcid.org/0009-0001-1951-1471">William Cagas</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/chan-ko-6a25a82b0/">Chan Ko</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://jonbarron.info">Blake Hsiao</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/shryuk-grandhi-52057531b/">Shryuk Grandhi</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/rishibhattacharya/">Rishi Bhattacharya</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Michael Lam</a><sup>6†</sup>,
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~seitz/">Kevin Zhu</a><sup>6†</sup>,
            </span>
            
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>St. Thomas More Catholic Secondary School,</span>
            <span class="author-block"><sup>2</sup>Staten Island Tech High School,</span>
            <span class="author-block"><sup>3</sup>Nutley High School,</span>
            <span class="author-block"><sup>4</sup>Canyon Crest Academy,</span>
            <span class="author-block"><sup>5</sup>The Judd School,</span>
            <span class="author-block"><sup>6</sup>University of California, Berkeley</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="contrib" style="margin-top: 15px; margin-bottom: 5px; display: inline-block;">
              <small><sup>*</sup>Lead Author</small>,
              <small><sup>†</sup>Senior Author</small>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="contrib" style="margin-top: 15px; margin-bottom: 5px; display: inline-block;">
              <small>williamgabriel.cagas@gmail.com, kevin@algoverse.us</small>
            </span>
          </div>
          
          <div class="is-size-5 publication-authors">
            <span class="contrib" style="margin-top: 15px; margin-bottom: 5px; display: inline-block;">
              <small><em>Published in the GAISynMeD Workshop at the <a href="https://accv2024.org/">Asian Conference on Computer Vision (ACCV) 2024</a></em></small>
            </span>
          </div>
          
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/ACCV2024W/GAISynMeD/papers/Cagas_Medical_Imaging_Complexity_and_its_Effects_on_GAN_Performance_ACCVW_2024_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <img src="/assets/CVF_Logo.png" style="width: 20px; height: 20px;" />
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2410.17959"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/willcagas/Synthetic-Medical-Image-Generation"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1Pu9AHaNkzr6nNqkcCjI9_Tmr9VgWfeC0/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <img src="/assets/Drive_Logo.png" style="width: 45px; height: 30px;" />
                  </span>
                  <span>Poster</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1RBwb-Dnxz9joZxnubcs4d5tawCsksltV/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <img src="/assets/Drive_Logo.png" style="width: 45px; height: 30px;" />
                  </span>
                  <span>Slides</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Problem Statement. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Problem Statement</h2>
        <div class="content has-text-justified">
          <p>
            The proliferation of machine learning models in diverse clinical applications has led to a growing need for high-quality medical data, often scarce due to patient privacy concerns as well as wet lab and data annotation cost constraints. <strong>Generative Adversarial Networks (GANs)</strong> try to address this problem by synthesizing medical images. However, the optimal training set sizes required to efficiently train a GAN to produce high-fidelity images are unknown. Existing methods primarily emphasize architecture-centric modifications to achieve desired outcomes while paying less attention to data-centric approaches.
          </p>
        </div>
      </div>
    </div>
    <!--/ Problem Statement. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Introduction -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Our Focus</h2>

        <div class="content has-text-justified">
          <p>
            1. We propose a <strong>data-centric optimization method</strong> for efficient GAN training in medical image synthesis using state-of-the-art GANs—StyleGAN 3 and SPADE-GAN.
          </p>

          <p>
            2. We base our GAN training guide on the relationship between the <strong>image complexity distribution</strong> of a dataset, and variable training set sizes that correspond to the fidelity of synthetic images.
          </p>  
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Background -->
    <div class="columns is-centered">
      <div class="column is-four-fifths">

        <!-- GANs -->
        <h2 class="title is-3 has-text-centered">Generative Adversarial Networks (GANs)</h2>
        <div class="content has-text-justified">
          <p>
            GANs are a class of generative models that consist of two neural networks: a generator \( G \), which aims to transform its latent variable distribution \( p⁢(z) \) to closely resemble the training data distribution \( p⁢(x) \), and a discriminator \( D \), which differentiates between the ground truth and data generated by \( G \). Training is an adversarial process where \( G \) attempts to deceive \( D \) into classifying its outputs as real. This two-player minimax game is represented by the following loss function:
          </p>
          
          <!-- Loss Function -->
          <p>
            \[
            \min_{G} \max_{D} V(D, G) = \mathbb{E}_{x \sim p(x)} [\log D(x)] + \mathbb{E}_{z \sim p(z)} [\log (1−D(G(z)))]
            \]
          </p>
        </div>

      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Background -->
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Measuring Image Complexity</h2>

        <div class="content has-text-justified">
          <p>
            Objectively, image complexity can be defined as the variety of features and details within an image. <strong>Entropy</strong> is a traditional, heuristic-based method of calculating the complexities of images in small-scale datasets. We consider three entropy-based metrics, and reason one to be used as the sole measure of complexity for each image in a medical dataset, with the ultimate goal of finding the image complexity distribution.
          </p>

          <h5 class="title is-5 has-text-left" style="margin-top: 25px; margin-bottom: 0.5em;">1. Shannon Entropy</h5>
          <p>
            Measures the uncertainty or “surprise” in data, specifically, the variation in the distribution of pixel intensities of an image in grayscale format. The equation is defined as
          </p>

          <!-- Entropy Formula -->
          <p>
            \[
            H = -\sum_{i=0}^{n-1} p_{i} \log_{b}(p_{i}),
            \]
          </p>

          <p>
            where \( n \) denotes the number of gray levels (256 for 8-bit images), \( b \) stands for the logarithmic base (returning bits when \( b=2 \)), and \( p_i \) is the probability of a pixel having gray level \( i \).
          </p>

          <p>
            <strong><mark style="background-color: orangered;">Problem:</mark></strong> Although Shannon entropy considers compositional image information, it fails to account for spatial information, specifically the relationship between neighbouring pixels.
          </p>

          <h5 class="title is-5 has-text-left" style="margin-top: 25px; margin-bottom: 0.5em;">2. Gray Level Co-Occurence Matrix (GLCM) Entropy</h5>
          <p>
            Unlike Shannon entropy, measures the randomness between pixel pairs in a grayscale image distribution, therefore considering spatial information as well. Similarly, it is calculated as
          </p>

          <!-- GLCM Entropy Formula -->
          <p>
            \[
            H_{g} = -\sum_{i=0}^{n-1} \sum_{j=0}^{n-1} p(i, j) \log_{b} (p(i, j)),
            \]
          </p>

          <p>
            where \( p(i,j) \)  is the probability of two pixels having gray levels \( i \) and \( j \) at a certain angle \( θ \) and distance \( d \) away from each other. 
          </p>

          <p>
            <strong><mark style="background-color: orangered;">Problem:</mark></strong> Despite GLCM better-capturing complexities within an image, it does not consider spatial patterns and pixel relationships beyond its adjacent pairing.
          </p>

          <h5 class="title is-5 has-text-left" style="margin-top: 25px; margin-bottom: 0.5em;">3. Delentropy</h5>
          <p>
            Incorporates a new density function known as the <strong>deledensity</strong>. As a joint probability function, it is formulated as
          </p>

          <!-- Deledensity Formula -->
          <p>
            \[
            p(i,j) = \frac{1}{4 W H} \sum_{w=0}^{W-1} \sum_{h=0}^{H-1} \delta_{i,d_{x}}(w,h) \delta_{j,d_{y}}(w,h),
            \]
          </p>

          <p>
            where \( d_{x} \) and \( d_{y} \) denote the derivative kernels in the \( x \) and \( y \) direction, \( \delta \) is the Kronecker delta to describe the binning operation required to generate a histogram, and \( H \) and \( W \) are the image’s dimensions (height and width). By obtaining this, we can then calculate delentropy as
          </p>

          <!-- Delentropy Formula -->
          <p>
            \[
            DE = -\frac{1}{2} \sum_{i=0}^{I-1} \sum_{j=0}^{J-1} p(i,j) \log_{b} p(i,j),
            \]
          </p>

          <p>
            such that \( I \) and \( J \) represent the number of bins (discrete cells) in the 2D distribution, and the \( \frac{1}{2} \) is derived from Papoulis’ generalized sampling expansion.
          </p>

          <p>
            <strong><mark style="background-color: greenyellow;">Solution:</mark></strong> By analyzing the relationship between the local and global features of an image, delentropy accounts for both an image’s gradient vector field and pixel co-occurence, encapsulating both its compositional and spatial information as a whole.
          </p>

          <p>
            To interpret this measure, yielding a <strong>high delentropy</strong> suggests an image has more sophisticated details, therefore being <strong>more complex</strong>. A <strong>low delentropy</strong> indicates a <strong>simple structure</strong> and a less-detailed image.
          </p>

        </div>

      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Datasets -->
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Datasets</h2>

        <div class="content has-text-justified">
          <p>
            We employed <strong>three medical image datasets</strong>  chosen for their diversity in both perceptual complexity and imaging modality—dermascopy, x-ray, and colonoscopy.
          </p>

          <p>
            1. <a href="https://challenge.isic-archive.com/data/#2018">International Skin Imaging Collaboration 2018 Challenge (ISIC-2018)</a>
          </p>

          <p>
            2. <a href="https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia">Chest X-Ray Images (Chest X-ray)</a>
          </p>

          <p>
            3. <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/FCBUOR">Colonoscopy Polyp Detection and Classification (Polyps Set)</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- GAN Pipeline -->
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">GAN Pipeline</h2>
        <div class="content has-text-justified">
          <p>
            <strong><mark style="background-color: yellow;">Goal:</mark></strong> To identify the role of image dataset size in the fidelity of generated images from Style GAN 3 and SPADE-GAN, for which to be compared to the delentropy (image complexity) distribution of each dataset.  
          </p>
        </div>
      </div>
    </div>

    <!-- Figure -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths has-text-justified">
        <figure class="image" style="margin-top: 20px; margin-bottom: 10px;">
          <img src="assets/Synthesized_Images.png" alt="Synthesized Images Comparison" style="width: 80%; height: auto; display: block; margin: auto;">
          <figcaption style="padding-top: 25px;"><em><strong>Figure 1:</strong> Comparison between original images and synthetic images from StyleGAN 3 and SPADE-GAN based on variable image set sizes.</em></figcaption>
        </figure>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <h5 class="title is-5 has-text-left" style="margin-top: 0px; margin-bottom: 0.5em;">Preprocessing</h5>
          <p>
            We first set all training images to a consistent <strong>512x512 resolution</strong>. As such, <strong>training parameters were based on the size of the preprocessed images</strong>, as documented in the official implementations. SPADE-GAN relies on segmentation masks, so we utilized pre-existing annotations or generated masks using TorchXRayVision.
          </p>

          <h5 class="title is-5 has-text-left" style="margin-top: 25px; margin-bottom: 0.5em;">Training and Generation</h5>
          <p>
            Through our data-centric approach, StyleGAN 3 and SPADE-GAN were run with the official, publicly available implementations with default hyperparameters and no augmentations to each network’s architecture. For each GAN training run, image (training) set size was subsequently set to <strong>500 images</strong> (baseline), <strong>1000 images</strong> (middle ground), and <strong>2500 images</strong> (comprehensive), randomly sampled from the same dataset for each experimental run, respectively. For StyleGAN 3, all experimental runs were trained for 100 epochs; for SPADE-GAN, training iterated 50 epochs. The trained GAN was then used to generate synthetic medical images, the fidelity of which was then calculated for each training set size. All experiments were performed on one NVIDIA A100 and three NVIDIA A40 GPUs.
          </p>

          <h5 class="title is-5 has-text-left" style="margin-top: 25px; margin-bottom: 0.5em;">Evaluation</h5>
          <p>
            The <strong>Fréchet Inception Distance (FID)</strong> was used to evaluate the fidelity of the synthetically generated images for GANs (i.e. GAN performance) for each training set. It is defined as the distance between the distributions of the ground truth and the generated images. A <strong>lower FID score</strong> signifies that a GAN is <strong>more proficient</strong> at generating synthetic data close to its target distribution. From this, we obtained fidelity curves for each dataset that describe how FID scores trend with increasing training set size shown below.
          </p>
        </div>
      </div>
    </div>

    <!-- Figure -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths has-text-justified">
        <figure class="image" style="margin-top: 20px; margin-bottom: 0px;">
          <img src="assets/FID_Scores_Comparison.png" alt="FID Scores Comparison">
          <figcaption style="padding-top: 25px;"><em><strong>Figure 2:</strong> Fréchet Inception Distance (FID) curves comparing StyleGAN 3 and SPADE-GAN across each medical image dataset with varying sample sizes. Lower FID scores correspond to higher fidelity synthetic images.</em></figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">

    <!-- Results -->
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Results</h2>

        <div class="content has-text-justified">
          <h5 class="title is-5 has-text-left" style="margin-top: 25px; margin-bottom: 0.5em;">GAN Performance Comparison</h5>
          <p>
            Predictably, FID scores consistently decreased with increasing dataset size.
          </p>
          <p>
            StyleGAN 3 trained on 2500 images reduced FID scores by a <strong>48%</strong> average, compared to when trained on 500 images. SPADE-GAN experienced an analogous <strong>31%</strong> FID score reduction on average.
            <strong>SPADE-GAN outperformed StyleGAN 3 across all datasets and training sizes, with FID scores averaging 33% lower.</strong>
          </p>
          <p>
            <strong><mark>Reason:</mark></strong> SPADE-GAN’s architecture used segmentation masks for extra structural information, compared to StyleGAN 3 which relied on raw data.
          </p>

          <h5 class="title is-5 has-text-left" style="margin-top: 25px; margin-bottom: 0.5em;">Delentropy Distributions</h5>

          <p>
            The graphs below show the distribution of image complexities (delentropies) for each image in each medical image dataset, respectively.
          </p>
        </div>
    
      </div>
    </div>

    <!-- Figure -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths has-text-justified">
        <figure class="image" style="margin-top: 0px; margin-bottom: 10px;">
          <img src="assets/Delentropy_Comparison.png" alt="Delentropy Distribution Comparison">
          <figcaption style="padding-top: 25px;"><em><strong>Figure 3:</strong> Delentropy distributions across each medical image dataset. A higher mean delentropy \( \mu \) indicates a dataset with more complex images.</em></figcaption>
        </figure>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-four-fifths">

        <div class="content has-text-justified">
          <p>
            The Chest X-ray dataset (homogeneous distribution) yields the <strong>lowest FID scores</strong>—indicates that GANs had <strong>easier training runs</strong>  with this dataset. The Polyps Set (widest distribution and more complex images) correlates with the <strong>highest FID scores</strong>—suggests that GANs had more <strong>challenging training runs</strong> with this dataset. 
          </p>
          <p>
            <strong><mark>Insight:</mark></strong> Shows an inverse relationship—GAN performance decreases with an increasing spread of image complexities within a dataset.
          </p>
          <h5 class="title is-5 has-text-left" style="margin-top: 25px; margin-bottom: 0.5em;">FID Curve Analysis</h5>
          <p>
            SPADE-GAN outperformed StyleGAN 3 with lower FID scores and smoother, non-overlapping FID curves. Performance plateaued after 1000 images, with little perceptual improvement in generated images as shown in the comparison of synthetic images. StyleGAN 3's performance did not plateau between 500 and 2500 images. FID scores showed increasingly negative slopes, indicating improved feature capture beyond 1000 images.
          </p>
          <p>
            <strong><mark>Insight:</mark></strong> FID curves serve as benchmarks for datasets with comparable delentropy distributions and SOTA GAN models.
          </p>
        </div>
    
      </div>
    </div>
  </div>
</section>

<!-- Limitations -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Limitations</h2>
        <div class="content has-text-justified">
          <p>
            Due to limited resources, training set sizes were constraint to only 500, 1000, and 2500 images leading to coarse-grained results.
          </p>
          <p>
            FID was used as the sole evaluation metric—may not be a good measure of how well synthetic images perform on a downstream task.
          </p>
          <p>
            A study using other generative AI such as <strong>VAEs</strong>, <strong>autoregressive models</strong>, and <strong>diffusion models</strong> may provide more significant insights.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Contributions -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Contributions</h2>
        <div class="content has-text-justified">
          <p>
            1. We empirically prove that higher image complexity leads to poorer image fidelity and lesser performance in GANs.
          </p>
          <p>
            2. We demonstrate that given a dataset of a similar delentropy distribution, healthcare professionals can reference our benchmarks of the closest image fidelity curve to gain an estimate of the optimal training set sizes to produce desired image quality results.
          </p>
          <p>
            3. We show the potential for studies using a data-centric approach with image complexity as a guide for model training—more thorough experiments are required before a truly comprehensive representation can be reached.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Acknowledgements -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Acknowledgements</h2>
        <div class="content has-text-justified">
          <p>
            We are grateful to Michael Lam and Kevin Zhu for their excellent mentorship, constructive feedback, and unwavering support throughout our research.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@InProceedings{Cagas_2024_ACCV,
    author    = {Cagas, William and Ko, Chan and Hsiao, Blake and Grandhi, Shryuk and Bhattacharya, Rishi and Zhu, Kevin and Lam, Michael},
    title     = {Medical Imaging Complexity and its Effects on GAN Performance},
    booktitle = {Proceedings of the Asian Conference on Computer Vision (ACCV) Workshops},
    month     = {December},
    year      = {2024},
    pages     = {207-217}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            The source code from this website was adapted from this <a
            href="https://github.com/nerfies/nerfies.github.io">template</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
